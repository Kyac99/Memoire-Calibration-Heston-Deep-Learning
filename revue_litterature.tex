\chapter{Revue de littérature}

\section{Introduction}

La calibration des modèles de volatilité stochastique constitue l'un des défis majeurs de la finance quantitative moderne. L'évolution des marchés financiers et l'émergence de nouveaux paradigmes technologiques ont créé un besoin pressant d'outils de calibration plus efficaces et précis. Cette revue de littérature présente l'état de l'art des méthodes de calibration traditionnelles et explore les développements récents liés à l'application du Deep Learning dans ce domaine.

\section{Modèles de volatilité stochastique : fondements théoriques}

\subsection{Le modèle de Black-Scholes et ses limitations}

Le modèle de Black-Scholes \citep{black1973pricing}, bien qu'ayant révolutionné la théorie de l'évaluation des options, repose sur l'hypothèse restrictive d'une volatilité constante. Cette assumption se révèle inadéquate face aux faits stylisés observés sur les marchés financiers, notamment le phénomène de smile de volatilité et la clustering de volatilité \citep{cont2001empirical}.

\subsection{L'émergence des modèles de volatilité stochastique}

Face aux limitations du modèle de Black-Scholes, les chercheurs ont développé des modèles incorporant la nature stochastique de la volatilité. Le modèle de Heston \citep{heston1993closed} constitue l'une des contributions les plus significatives de cette période, proposant une solution analytique pour le pricing d'options européennes dans un cadre de volatilité stochastique.

Le modèle de Heston modélise l'évolution conjointe du prix de l'actif sous-jacent $S_t$ et de sa variance instantanée $v_t$ selon les équations différentielles stochastiques suivantes :

\begin{align}
dS_t &= rS_t dt + \sqrt{v_t}S_t dW_t^S \\
dv_t &= \kappa(\theta - v_t)dt + \sigma \sqrt{v_t}dW_t^v
\end{align}

où $dW_t^S$ et $dW_t^v$ sont des mouvements browniens corrélés avec $d\langle W^S, W^v \rangle_t = \rho dt$.

Les paramètres du modèle sont : $\kappa$ (vitesse de retour à la moyenne), $\theta$ (niveau de long terme de la variance), $\sigma$ (volatilité de la volatilité), $\rho$ (corrélation entre les innovations de prix et de volatilité), et $v_0$ (variance initiale).

\subsection{Développements récents : modèles de volatilité rugueuse}

Les travaux de \citet{gatheral2018volatility} ont révolutionné la compréhension de la dynamique de volatilité en démontrant empiriquement que les séries de volatilité réalisée présentent des propriétés de rugosité, caractérisées par un exposant de Hurst $H \approx 0.1$. Cette découverte a mené au développement des modèles de volatilité rugueuse, notamment le modèle rough Bergomi (rBergomi) de \citet{bayer2016pricing}.

Le modèle rBergomi s'écrit :

\begin{align}
S_t &= S_0 \exp\left(rt + \int_0^t \sqrt{v_s} dW_s^1 - \frac{1}{2}\int_0^t v_s ds\right) \\
v_t &= \xi_0(t) \mathcal{E}\left(\int_0^t \sqrt{v_s} dW_s^2\right)
\end{align}

où $\xi_0(t)$ est un processus gaussien rugueux et $\mathcal{E}$ désigne l'exponentielle stochastique.

\section{Méthodes de calibration traditionnelles}

\subsection{Calibration par optimisation}

Les méthodes traditionnelles de calibration reposent sur la résolution de problèmes d'optimisation visant à minimiser l'écart entre les prix de marché observés et les prix théoriques du modèle. L'objectif typique s'écrit :

\begin{equation}
\hat{\theta} = \arg\min_{\theta} \sum_{i=1}^{N} w_i \left(C_i^{market} - C_i^{model}(\theta)\right)^2
\end{equation}

où $C_i^{market}$ représente le prix de marché de l'option $i$, $C_i^{model}(\theta)$ le prix théorique avec les paramètres $\theta$, et $w_i$ les poids associés.

\citet{cui2017full} ont développé des techniques d'optimisation spécialisées pour la calibration du modèle de Heston, démontrant l'importance du choix des conditions initiales et des contraintes de paramètres pour éviter les minima locaux.

\subsection{Défis computationnels}

La calibration traditionnelle présente plusieurs limitations majeures. Premièrement, le calcul répétitif des prix d'options requiert l'évaluation de fonctions caractéristiques complexes ou de simulations Monte Carlo coûteuses \citep{lord2010comparison}. Deuxièmement, les algorithmes d'optimisation sont sensibles aux conditions initiales et peuvent converger vers des minima locaux \citep{mikhailov2003heston}.

Pour les modèles de volatilité rugueuse, ces défis s'intensifient considérablement. L'absence de solutions analytiques pour la plupart des modèles rugueux nécessite l'utilisation de simulations Monte Carlo ou de méthodes d'approximation, rendant la calibration prohibitivement coûteuse \citep{mccrickerd2018turbocharging}.

\subsection{Approches par transformation de Fourier}

\citet{carr1999option} ont développé des méthodes efficaces basées sur la transformation de Fourier pour le pricing d'options dans les modèles de volatilité stochastique. Ces techniques exploitent la disponibilité de fonctions caractéristiques analytiques pour accélérer significativement le calcul des prix. Cependant, même avec ces optimisations, la calibration reste computationnellement intensive.

\section{L'avènement du Machine Learning en finance}

\subsection{Précurseurs historiques}

L'application du Machine Learning à la finance quantitative n'est pas récente. \citet{hutchinson1994nonparametric} furent parmi les premiers à explorer l'utilisation de réseaux de neurones pour le pricing et la couverture de produits dérivés. Leurs travaux démontrèrent la capacité des réseaux de neurones à approximer des fonctions de pricing complexes.

\subsection{Renaissance du Deep Learning}

La renaissance du Deep Learning, catalysée par les avancées en puissance de calcul et en algorithmes d'optimisation, a ouvert de nouvelles perspectives. \citet{sirignano2018dgm} ont développé la méthode DGM (Deep Galerkin Method) pour résoudre des équations aux dérivées partielles financières, tandis que \citet{beck2017deep} ont proposé des approches basées sur les réseaux de neurones profonds pour les équations aux dérivées partielles de haute dimension.

\section{Deep Learning pour la calibration de modèles : approches émergentes}

\subsection{L'approche pionnière de Hernandez}

\citet{hernandez2017model} a publié le travail pionnier qui a relancé l'intérêt pour l'application du Deep Learning à la calibration de modèles financiers. Son approche consistait à entraîner directement un réseau de neurones à prédire les paramètres de modèle à partir des données de marché observées.

\subsection{Calibration en deux étapes : la méthodologie de Bayer et Stemper}

\citet{bayer2018deep} ont proposé une approche révolutionnaire en deux étapes pour la calibration de modèles de volatilité stochastique. Leur méthodologie se distingue par sa philosophie : plutôt que d'apprendre directement les paramètres de calibration, ils proposent d'apprendre la fonction de pricing elle-même.

La première étape consiste à entraîner un réseau de neurones $\mathcal{N}_\phi$ à approximer la fonction de mapping des paramètres vers les volatilités implicites :

\begin{equation}
\mathcal{N}_\phi : \Theta \times \mathcal{M} \times \mathcal{T} \rightarrow \mathbb{R}^+
\end{equation}

où $\Theta$ représente l'espace des paramètres du modèle, $\mathcal{M}$ l'espace des moneyness, et $\mathcal{T}$ l'espace des maturités.

La seconde étape utilise cette approximation neuronale dans un algorithme de calibration traditionnel, typiquement Levenberg-Marquardt, remplaçant les évaluations coûteuses de Monte Carlo par des évaluations rapides du réseau de neurones.

Cette approche présente plusieurs avantages significatifs. Elle préserve la robustesse des algorithmes d'optimisation établis tout en bénéficiant de l'efficacité computationnelle des réseaux de neurones. De plus, elle permet une meilleure interprétabilité et contrôle du processus de calibration.

\subsection{Extension aux modèles rugueux}

\citet{bayer2019deep} ont étendu leur méthodologie aux modèles de volatilité rugueuse, démontrant pour la première fois la faisabilité de la calibration en temps réel de modèles rBergomi. Leurs expériences numériques montrent une précision remarquable avec des gains de vitesse de plusieurs ordres de grandeur comparé aux méthodes traditionnelles.

Les auteurs introduisent des innovations techniques importantes, notamment l'utilisation de grilles aléatoires pour l'entraînement et des techniques de régularisation spécifiques pour gérer la haute dimensionnalité du problème de calibration de modèles rugueux.

\subsection{Approche par réseaux convolutionnels : la contribution de Stone}

\citet{stone2020calibrating} a développé une approche alternative utilisant des réseaux de neurones convolutionnels (CNN) pour la calibration directe de l'exposant de Hurst dans les modèles de volatilité rugueuse. Son approche est particulièrement innovante car elle traite les chemins de prix simulés comme des images, exploitant ainsi la capacité des CNN à identifier des motifs dans les données spatiales.

La méthodologie de Stone consiste à :
\begin{enumerate}
\item Simuler des chemins de prix sous différentes valeurs de l'exposant de Hurst H
\item Traiter ces chemins comme des signaux unidimensionnels
\item Entraîner un CNN à prédire la valeur de H à partir de ces signaux
\item Utiliser le réseau entraîné pour calibrer H sur des données réelles
\end{enumerate}

Cette approche démontre une précision remarquable dans l'estimation de l'exposant de Hurst, ouvrant la voie à de nouvelles applications des techniques de vision par ordinateur en finance quantitative.

\section{Techniques avancées et développements récents}

\subsection{Differential Machine Learning}

\citet{huge2020differential} ont introduit le concept de Differential Machine Learning (DML), une technique permettant d'apprendre simultanément une fonction et ses dérivées. Cette approche est particulièrement pertinente pour la finance quantitative où les sensibilités (grecques) sont aussi importantes que les prix eux-mêmes.

Le DML optimise conjointement la fonction de pricing et ses dérivées :

\begin{equation}
\min_\phi \sum_{i=1}^{N} \left[w_i^{(0)}(f_i - \mathcal{N}_\phi(\mathbf{x}_i))^2 + \sum_{j=1}^{d} w_i^{(j)}\left(\frac{\partial f_i}{\partial x_j} - \frac{\partial \mathcal{N}_\phi(\mathbf{x}_i)}{\partial x_j}\right)^2\right]
\end{equation}

Cette technique améliore significativement la précision des approximations neuronales, particulièrement dans les régions où les données d'entraînement sont rares.

\subsection{Réseaux Adverses Génératifs (GANs)}

\citet{cuchiero2020generative} ont exploré l'utilisation de réseaux adverses génératifs pour la calibration de modèles de volatilité locale stochastique. Leur approche génère synthétiquement des surfaces de volatilité implicite, permettant d'augmenter artificiellement la quantité de données d'entraînement.

\subsection{Physics-Informed Neural Networks (PINNs)}

Les PINNs, développés par \citet{raissi2019physics}, incorporent les équations gouvernantes directement dans la fonction de perte du réseau de neurones. Cette approche garantit que les approximations neuronales respectent les contraintes physiques ou financières du problème.

Pour la calibration de modèles de volatilité stochastique, les PINNs peuvent incorporer les équations aux dérivées partielles de Black-Scholes généralisées, assurant ainsi la cohérence arbitrage-free des prix prédits.

\section{Applications pratiques et études empiriques}

\subsection{Données de marché et validation empirique}

La validation empirique des méthodes de calibration par Deep Learning constitue un aspect crucial de la recherche. \citet{horvath2021deep} ont conduit des études approfondies sur données SPX, démontrant la supériorité des approches neuronales en termes de vitesse et de précision.

Les études empiriques révèlent que les méthodes de Deep Learning maintiennent leur précision même dans des conditions de marché volatiles, où les méthodes traditionnelles peuvent échouer à converger.

\subsection{Gestion des risques et applications en temps réel}

L'application pratique de ces méthodes en environnement de trading nécessite une attention particulière aux aspects de robustesse et de stabilité. \citet{ruf2020neural} ont développé des frameworks pour l'intégration de modèles neuronaux dans les systèmes de gestion des risques en temps réel.

\section{Défis et limitations}

\subsection{Interprétabilité et transparence}

L'un des défis majeurs de l'utilisation du Deep Learning en finance quantitative concerne l'interprétabilité des modèles. Les régulateurs financiers exigent de plus en plus de transparence dans les modèles de risque, créant une tension avec la nature "boîte noire" des réseaux de neurones profonds.

\citet{molnar2020interpretable} discutent diverses techniques pour améliorer l'interprétabilité des modèles de Machine Learning, incluant SHAP (SHapley Additive exPlanations) et LIME (Local Interpretable Model-agnostic Explanations).

\subsection{Robustesse et généralisation}

La robustesse des modèles neuronaux face aux changements de régime de marché constitue une préoccupation majeure. \citet{cont2020robustness} analysent la stabilité des modèles de Machine Learning dans différents environnements de marché.

\subsection{Données d'entraînement et overfitting}

La qualité et la représentativité des données d'entraînement influencent critiquement les performances des modèles neuronaux. Le risque d'overfitting est particulièrement préoccupant quand les données historiques ne capturent pas adéquatement la diversité des conditions de marché futures.

\section{Perspectives futures}

\subsection{Intelligence artificielle explicable (XAI)}

Le développement de techniques d'IA explicable spécifiquement adaptées à la finance quantitative représente une direction de recherche prometteuse. Ces techniques pourraient réconcilier la puissance prédictive du Deep Learning avec les exigences réglementaires de transparence.

\subsection{Apprentissage fédéré}

L'apprentissage fédéré pourrait permettre aux institutions financières de bénéficier collectivement de modèles améliorés sans partager directement leurs données sensibles, ouvrant ainsi de nouvelles possibilités de collaboration dans le développement de modèles de calibration.

\subsection{Calcul quantique}

L'émergence du calcul quantique pourrait révolutionner la calibration de modèles complexes. Les algorithmes quantiques promettent des accélérations exponentielles pour certains types de problèmes d'optimisation, potentiellement transformant l'approche de la calibration de modèles de volatilité stochastique.

\section{Conclusion}

La revue de littérature révèle une transformation profonde du paysage de la calibration de modèles de volatilité stochastique. Les approches de Deep Learning, particulièrement celles développées par Bayer et Stemper d'une part, et Stone d'autre part, démontrent un potentiel remarquable pour surmonter les limitations computationnelles des méthodes traditionnelles.

L'approche en deux étapes de Bayer et Stemper offre un équilibre optimal entre innovation technologique et robustesse méthodologique, tandis que l'approche CNN de Stone ouvre de nouvelles perspectives pour l'exploitation de techniques de vision par ordinateur en finance.

Cependant, des défis significatifs demeurent, notamment concernant l'interprétabilité, la robustesse et la validation réglementaire de ces nouvelles méthodologies. La recherche future devra probablement se concentrer sur le développement de techniques hybrides combinant la puissance du Deep Learning avec la transparence et la robustesse requises pour les applications financières critiques.

Cette évolution s'inscrit dans une tendance plus large de digitalisation de la finance, où l'efficacité computationnelle devient un avantage concurrentiel déterminant. Les institutions qui maîtriseront ces nouvelles technologies de calibration seront mieux positionnées pour gérer les risques et saisir les opportunités dans un environnement de marché de plus en plus complexe et volatil.
