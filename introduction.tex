\chapter{Introduction}

\section{Contexte et motivation}

La calibration des modèles de volatilité stochastique constitue l'un des défis techniques les plus complexes de la finance quantitative moderne. Dans un environnement de marché caractérisé par une volatilité croissante et des structures de données de plus en plus riches, les institutions financières font face à des exigences contradictoires : d'une part, la nécessité de calibrer des modèles sophistiqués avec une précision et une fréquence accrues, et d'autre part, les contraintes computationnelles qui limitent l'application pratique de ces approches avancées.

Le modèle de Heston \citep{heston1993closed}, introduit en 1993, demeure l'un des piliers de la modélisation de volatilité stochastique en finance quantitative. Sa popularité repose sur l'existence de solutions analytiques pour le pricing d'options européennes, sa capacité à reproduire le phénomène de smile de volatilité observé sur les marchés, et sa tractabilité mathématique qui permet l'implémentation de techniques de calibration robustes. Cependant, malgré ces avantages théoriques, la calibration pratique du modèle de Heston présente des défis computationnels significatifs qui limitent son utilisation en environnement de trading haute fréquence.

L'émergence des options SPX Weekly, introduites par le Chicago Board Options Exchange (CBOE) en 2005, a transformé le paysage de la gestion des risques et du trading d'options. Ces instruments, caractérisés par des maturités très courtes (typiquement 1 à 60 jours), présentent des défis uniques pour la calibration de modèles. La granularité temporelle fine requiert des recalibrations fréquentes, tandis que la liquidité élevée impose des contraintes de précision particulièrement strictes.

Dans ce contexte, l'avènement du Deep Learning offre des perspectives révolutionnaires pour surmonter les limitations computationnelles traditionnelles. Les travaux pionniers de \citet{bayer2018deep} ont démontré qu'il est possible de remplacer les évaluations coûteuses de Monte Carlo par des approximations neuronales rapides et précises, ouvrant ainsi la voie à une nouvelle génération d'outils de calibration.

\section{Problématique et enjeux}

La problématique centrale de cette recherche s'articule autour d'une question fondamentale : \textbf{dans quelle mesure un réseau de Deep Learning peut-il remplacer ou accélérer la calibration traditionnelle du modèle de Heston sur des données réelles, tout en maintenant voire améliorant la précision et la robustesse requises pour les applications financières critiques ?}

Les méthodes traditionnelles de calibration du modèle de Heston reposent sur l'optimisation itérative de fonctions objectif complexes, nécessitant l'évaluation répétée de formules de pricing impliquant des intégrales de fonctions caractéristiques. Cette approche présente des limitations majeures : complexité temporelle élevée, sensibilité aux conditions initiales, et instabilité numérique dans certaines régions de l'espace des paramètres.

Ces limitations deviennent particulièrement critiques dans le contexte des options SPX Weekly, où la nécessité de recalibrations fréquentes (potentiellement intra-day) rend les approches traditionnelles impraticables pour de nombreuses applications. Au-delà des considérations computationnelles, la question de la précision revêt une importance capitale, les erreurs de calibration se propageant directement dans les calculs de prix et de risques.

\section{Objectifs de la recherche}

Cette recherche vise à \textbf{démontrer empiriquement la viabilité et la supériorité de l'approche de calibration par Deep Learning pour le modèle de Heston}, en s'appuyant sur une validation rigoureuse utilisant des données de marché réelles SPX Weekly.

Les objectifs spécifiques incluent :
\begin{itemize}
\item Implémenter et valider l'approche en deux étapes de \citet{bayer2018deep}
\item Quantifier précisément les gains de performance en termes de vitesse et précision
\item Analyser la stabilité temporelle des paramètres calibrés à travers différentes conditions de marché
\item Évaluer la qualité des grecques calculées via l'approche neuronale
\item Tester la robustesse face aux variations de liquidité et aux événements de marché exceptionnels
\end{itemize}

\section{Hypothèses de recherche}

Cette étude repose sur trois hypothèses principales :

\textbf{H1 - Accélération computationnelle :} L'approche neuronale permettra une réduction d'au moins un ordre de grandeur des temps de calibration par rapport aux méthodes traditionnelles, tout en maintenant une précision équivalente.

\textbf{H2 - Robustesse supérieure :} Les réseaux de neurones démontreront une stabilité et un taux de convergence supérieurs aux algorithmes d'optimisation traditionnels, particulièrement dans des conditions de marché volatiles.

\textbf{H3 - Généralisation temporelle :} Les modèles entraînés sur des données historiques maintiendront leur performance sur des données out-of-sample, démontrant leur capacité de généralisation à de nouvelles conditions de marché.

\section{Structure du mémoire}

Ce mémoire s'organise en six chapitres principaux qui progressent logiquement de la contextualisation théorique vers l'application pratique.

Le \textbf{Chapitre 2} établit le contexte théorique en analysant l'évolution des modèles de volatilité stochastique et l'émergence des approches de Deep Learning. Le \textbf{Chapitre 3} détaille les caractéristiques du dataset SPX Weekly et les procédures de préprocessing. Le \textbf{Chapitre 4} présente l'implémentation de l'approche en deux étapes, incluant l'architecture neuronale et l'intégration avec les algorithmes d'optimisation traditionnels.

Le \textbf{Chapitre 5} constitue le cœur empirique du mémoire, présentant les résultats des expérimentations et les analyses comparatives. Le \textbf{Chapitre 6} synthétise les contributions principales et identifie les directions de recherche future prometteuses. Les \textbf{Annexes} fournissent les détails techniques nécessaires à la reproduction des résultats.

Cette recherche s'inscrit dans une transformation plus large de l'industrie financière vers une digitalisation accrue et une adoption généralisée de l'intelligence artificielle. L'enjeu dépasse la simple optimisation technique : il s'agit de transformer la manière dont l'industrie financière aborde la modélisation, la calibration et la gestion des risques dans un environnement de plus en plus complexe et dynamique.
