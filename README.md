# MÃ©moire : Calibration accÃ©lÃ©rÃ©e du modÃ¨le de Heston par Deep Learning

[![LaTeX](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)](https://www.latex-project.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Description

Ce mÃ©moire prÃ©sente une recherche approfondie sur l'application du Deep Learning Ã  la calibration du modÃ¨le de Heston pour le pricing d'options. L'Ã©tude dÃ©montre comment les techniques d'apprentissage automatique peuvent rÃ©volutionner la calibration traditionnelle, offrant une accÃ©lÃ©ration spectaculaire (facteur 11x) tout en amÃ©liorant la prÃ©cision et la robustesse.

**Titre complet :** *Calibration accÃ©lÃ©rÃ©e du modÃ¨le de Heston par Deep Learning : conception, implÃ©mentation et benchmarks sur donnÃ©es SPX Weekly*

## ğŸ›ï¸ Contexte acadÃ©mique

Cette recherche s'appuie sur les travaux fondamentaux de :
- **Bayer & Stemper (2018)** : "Deep calibration of rough stochastic volatility models"
- **Stone (2019)** : "Calibrating rough volatility models: a convolutional neural network approach"

L'Ã©tude adapte et Ã©tend ces mÃ©thodologies au contexte spÃ©cifique du modÃ¨le de Heston et des donnÃ©es SPX Weekly.

## ğŸ“š Structure du mÃ©moire

Le mÃ©moire est organisÃ© en 6 chapitres principaux (70-80 pages) :

### Chapitre 1 : Introduction
- Contexte et motivation
- ProblÃ©matique centrale et enjeux
- Objectifs de recherche
- MÃ©thodologie gÃ©nÃ©rale
- Contributions attendues

### Chapitre 2 : Revue de littÃ©rature
- ModÃ¨les de volatilitÃ© stochastique : fondements thÃ©oriques
- MÃ©thodes de calibration traditionnelles
- L'avÃ¨nement du Machine Learning en finance
- Deep Learning pour la calibration : approches Ã©mergentes
- Techniques avancÃ©es (DML, GANs, PINNs)
- DÃ©fis et limitations

### Chapitre 3 : PrÃ©sentation des donnÃ©es
- CaractÃ©ristiques du dataset SPX Weekly
- ProcÃ©dures de nettoyage et prÃ©processing
- Analyse exploratoire
- SpÃ©cificitÃ©s des options Weekly

### Chapitre 4 : MÃ©thodologie
- Le modÃ¨le de Heston : fondements mathÃ©matiques
- ProcÃ©dures de calibration traditionnelles
- MÃ©thodologie Deep Learning (approche en deux Ã©tapes)
- Architecture du rÃ©seau de neurones
- ProcÃ©dures d'entraÃ®nement et validation
- Calibration hybride

### Chapitre 5 : RÃ©sultats et discussions
- Configuration expÃ©rimentale
- RÃ©sultats de l'entraÃ®nement
- Performance comparative sur donnÃ©es synthÃ©tiques
- Tests sur donnÃ©es de marchÃ© rÃ©elles
- Analyse des sensibilitÃ©s (grecques)
- Tests de robustesse et validation

### Chapitre 6 : Conclusions
- SynthÃ¨se des contributions
- Implications thÃ©oriques et pratiques
- Limitations et perspectives
- Recommandations d'implÃ©mentation
- Directions de recherche future

### Annexes
- DÃ©tails techniques de l'implÃ©mentation
- Code source des fonctions principales
- RÃ©sultats expÃ©rimentaux dÃ©taillÃ©s
- SpÃ©cifications d'architecture

## ğŸ”¬ MÃ©thodologie

### Approche en deux Ã©tapes

**Ã‰tape 1 - Apprentissage :** EntraÃ®nement d'un rÃ©seau de neurones pour approximer la fonction de mapping : paramÃ¨tres Heston â†’ volatilitÃ©s implicites

**Ã‰tape 2 - Calibration :** Utilisation de cette approximation dans un algorithme d'optimisation traditionnel (Levenberg-Marquardt)

### Avantages clÃ©s
- âš¡ **AccÃ©lÃ©ration :** Facteur 11x par rapport aux mÃ©thodes traditionnelles
- ğŸ¯ **PrÃ©cision :** AmÃ©lioration de 12.5% en RMSE sur donnÃ©es rÃ©elles
- ğŸ›¡ï¸ **Robustesse :** Taux de convergence de 98% vs 87% (traditionnel)
- ğŸ”„ **CompatibilitÃ© :** IntÃ©gration transparente dans les workflows existants

## ğŸ“Š RÃ©sultats principaux

### Performance computationnelle
| MÃ©thode | Temps moyen | Taux de convergence | RMSE paramÃ¨tres |
|---------|-------------|-------------------|------------------|
| Traditionnelle | 23.4 s | 87% | 0.092 |
| Deep Learning | 2.1 s | 98% | 0.089 |
| **AmÃ©lioration** | **11.1x** | **+11 pp** | **-3.3%** |

### Validation sur donnÃ©es SPX Weekly (2020-2022)
- **RMSE IV :** 0.0251 vs 0.0287 (traditionnel) - AmÃ©lioration de 12.5%
- **MAE IV :** 0.0171 vs 0.0198 (traditionnel) - AmÃ©lioration de 13.6%
- **RÂ² pricing :** 0.9389 vs 0.9341 (traditionnel)

## ğŸ› ï¸ Technologies utilisÃ©es

- **LaTeX** : RÃ©daction du mÃ©moire
- **Python** : ImplÃ©mentation (rÃ©fÃ©rence conceptuelle)
- **PyTorch** : Framework Deep Learning
- **CUDA** : Calcul GPU haute performance
- **NumPy/SciPy** : Calculs numÃ©riques
- **Git** : ContrÃ´le de version

## ğŸ“ Organisation des fichiers

```
â”œâ”€â”€ memoire_principal.tex        # Document principal LaTeX
â”œâ”€â”€ introduction.tex             # Chapitre 1
â”œâ”€â”€ revue_litterature.tex       # Chapitre 2
â”œâ”€â”€ presentation_donnees.tex    # Chapitre 3
â”œâ”€â”€ methodologie.tex            # Chapitre 4
â”œâ”€â”€ resultats_discussions.tex   # Chapitre 5
â”œâ”€â”€ conclusions.tex             # Chapitre 6
â”œâ”€â”€ annexes.tex                 # Annexes techniques
â”œâ”€â”€ references.bib              # Bibliographie
â””â”€â”€ README.md                   # Ce fichier
```

## ğŸš€ Compilation LaTeX

### PrÃ©requis
- Distribution LaTeX complÃ¨te (TeX Live, MiKTeX, etc.)
- Packages requis : voir `memoire_principal.tex`

### Compilation
```bash
# Compilation complÃ¨te avec bibliographie
pdflatex memoire_principal.tex
bibtex memoire_principal
pdflatex memoire_principal.tex
pdflatex memoire_principal.tex

# Ou avec latexmk (recommandÃ©)
latexmk -pdf -pvc memoire_principal.tex
```

## ğŸ“– Guide de lecture

### Pour les acadÃ©miciens
- Focus sur la **revue de littÃ©rature** (Chapitre 2) et la **mÃ©thodologie** (Chapitre 4)
- Attention particuliÃ¨re aux **tests de validation** (Chapitre 5)

### Pour les praticiens
- Concentration sur les **rÃ©sultats pratiques** (Chapitre 5)
- **Recommandations d'implÃ©mentation** (Chapitre 6)
- **Annexes techniques** pour les dÃ©tails d'implÃ©mentation

### Pour les Ã©tudiants
- Lecture sÃ©quentielle recommandÃ©e
- **Introduction** pour le contexte gÃ©nÃ©ral
- **MÃ©thodologie** pour la comprÃ©hension technique

## ğŸ”— RÃ©fÃ©rences principales

```bibtex
@article{bayer2018deep,
  title={Deep calibration of rough stochastic volatility models},
  author={Bayer, Christian and Stemper, Benjamin},
  journal={arXiv preprint arXiv:1810.03399},
  year={2018}
}

@article{stone2020calibrating,
  title={Calibrating rough volatility models: a convolutional neural network approach},
  author={Stone, Henry},
  journal={Quantitative Finance},
  volume={20},
  number={3},
  pages={379--392},
  year={2020}
}

@article{heston1993closed,
  title={A closed-form solution for options with stochastic volatility},
  author={Heston, Steven L},
  journal={The Review of Financial Studies},
  volume={6},
  number={2},
  pages={327--343},
  year={1993}
}
```

## ğŸ’¡ Contributions principales

### ThÃ©oriques
- âœ… Validation empirique rigoureuse de l'approche Bayer-Stemper
- âœ… Extension aux spÃ©cificitÃ©s des options SPX Weekly
- âœ… Analyse comparative exhaustive avec mÃ©thodes traditionnelles
- âœ… Frameworks de validation pour modÃ¨les hybrides

### Pratiques
- âœ… DÃ©monstration de faisabilitÃ© en environnement rÃ©aliste
- âœ… Quantification des bÃ©nÃ©fices Ã©conomiques
- âœ… Identification des bonnes pratiques d'implÃ©mentation
- âœ… Recommandations pour adoption industrielle

### Technologiques
- âœ… Framework complet de calibration hybride
- âœ… Outils de diagnostic et monitoring temps rÃ©el
- âœ… Benchmarks de rÃ©fÃ©rence reproductibles
- âœ… Protocoles de test standardisÃ©s

## ğŸ¯ Applications pratiques

### Trading haute frÃ©quence
- Recalibration intra-day en temps rÃ©el
- Gestion dynamique des risques
- Pricing adaptatif d'options complexes

### Gestion des risques
- Stress testing accÃ©lÃ©rÃ©
- Calcul de fonds propres optimisÃ©
- Validation de modÃ¨les amÃ©liorÃ©e

### ConformitÃ© rÃ©glementaire
- TraÃ§abilitÃ© et reproductibilitÃ© accrues
- Documentation automatisÃ©e des calibrations
- Analyses de sensibilitÃ© approfondies

## ğŸ”® Perspectives futures

### Court terme
- Extension Ã  d'autres modÃ¨les (SABR, volatilitÃ© locale)
- Optimisation des architectures neuronales
- DÃ©ploiement en environnement de production

### Long terme
- Apprentissage adaptatif aux conditions de marchÃ©
- IntÃ©gration dans des frameworks de gestion globale
- Contribution Ã  la finance quantitative moderne

## ğŸ“ Citation

Si vous utilisez ce travail dans vos recherches, veuillez le citer comme suit :

```bibtex
@mastersthesis{konseiga2025calibration,
  title={Calibration accÃ©lÃ©rÃ©e du modÃ¨le de Heston par Deep Learning : conception, implÃ©mentation et benchmarks sur donnÃ©es SPX Weekly},
  author={Konseiga, PÃªgdwendÃ© Yacouba},
  year={2025},
  school={[Nom de l'universitÃ©]},
  type={MÃ©moire de Master}
}
```

## ğŸ“ Contact

Pour toute question concernant ce mÃ©moire ou ses implÃ©mentations :

- **Auteur :** PÃªgdwendÃ© Yacouba KONSEIGA
- **Email :** [votre-email@universitÃ©.edu]
- **LinkedIn :** [Votre profil LinkedIn]
- **GitHub :** [@Kyac99](https://github.com/Kyac99)

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

**Note :** Ce mÃ©moire reprÃ©sente un travail de recherche acadÃ©mique. Les rÃ©sultats et recommandations sont fournis Ã  des fins Ã©ducatives et de recherche. Toute application pratique doit faire l'objet d'une validation indÃ©pendante et appropriÃ©e.
